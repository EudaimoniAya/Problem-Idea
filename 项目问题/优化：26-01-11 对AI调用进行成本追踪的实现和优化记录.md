## 总结
在完成日志功能的开发后，想到它能够通过日志装饰器实现在函数内部自由调用，于是试图完成企业级项目的一个必备模块——成本追踪的开发。在实现的过程中发现并解决了一些问题：
* 在实际需要提取信息时才意识到的计数（`input_token`、`output_token`、`total_token`等）在`AIMessage`和`ToolMessage`中的位置，不同大模型服务提供商可能会不同，如何设计一种通用的方法累计token消耗总数并正确地处理？
* token的价格不同厂商的不同模型价格不同，如何方便且可扩展地记录？
* 假设在视图层某个功能（函数）中调用了`invoke()`方法，成本分析只能够追踪外部的返回信息（如在返回的消息列表中一个个地累计），是否有方法能够深入Agent调用过程内部，在每个`BaseMessage`子类实例被创建出的时候就进行成本分析？


## 学习过程
### 一、`BaseMessage`及其子类中的token信息
#### 1. `Message`系列类的关系
LangChain框架就是通过几个抽象基类发散功能构成的，其中所有的消息类都继承自`BaseMessage`（流式输出除外，它们继承的是`BaseMessageChunk`），它们的主要区别在于角色`role`，这决定了消息在对话中的语义。
在`langchain_core/messages/__init__.py`中，暴露了所有的`Message`相关的类：

| 类名                  | 继承消息基类                          | 职能（源码注释）                                                                     | 位置                     |
| ------------------- | ------------------------------- | ---------------------------------------------------------------------------- | ---------------------- |
| **`SystemMessage`** | `BaseMessage`                   | 规范AI行为的消息，一般作为输入消息序列中的第一个消息                                                  | `messages/system.py`   |
| **`HumanMessage`**  | `BaseMessage`                   | 来自用户的消息                                                                      | `messages/human.py`    |
| **`AIMessage`**     | `BaseMessage`                   | 代表了AI的输出结果，包含模型返回的原始输出，以及LangChain框架添加的标准化字段（`tool_calls`、`usage_metadata`等） | `messages/ai.py`       |
| **`ToolMessage`**   | `BaseMessage` `ToolOutputMixin` | 传递工具执行结果的消息，通过`content`字段给出工具执行结果，通过`tool_call_id`，在多工具场景下将工具调用请求与工具调用结果联系   | `messages/tool.py`     |
| `FunctionMessage`   | `BaseMessage`                   | **是`ToolMessage`的过时版本**，且不包含`tool_call_id`字段                                 | `messages/function.py` |
| `ChatMessage`       | `BaseMessage`                   | 可任意分配角色的消息，有两个字段：`role`和`type`（消息类型）                                         | `messages/chat.py`     |
| `RemoveMessage`     | `BaseMessage`                   | 负责删除其他消息的消息                                                                  | `messages/modifier.py` |
| `AnyMessage`        | 无                               | 表示任何已定义的`Message`或`MessageChunk`类型                                           | `messages/utils.py`    |
可见最常用的几种消息：`SystemMessage`、`HumanMessage`、`AIMessage`都统一由一个抽象基类管理，而其派生出的`MessageChunk`类，则通过多继承的方式额外支持流式输出（`ToolMessage`例外）：

| 类名                     | 继承流式基类                                    | 职能（源码注释）                        | 位置                     |
| ---------------------- | ----------------------------------------- | ------------------------------- | ---------------------- |
| `SystemMessageChunk`   | `BaseMessageChunk`、`SystemMessageChunk`   | 流式输出时产生（yielded when streaming） | `messages/system.py`   |
| `HumanMessageChunk`    | `BaseMessageChunk`、`HumanMessageChunk`    | -                               | `messages/human.py`    |
| `AIMessageChunk`       | `BaseMessageChunk`、`AIMessage`            | -                               | `messages/ai.py`       |
| `ToolMessageChunk`     | `BaseMessageChunk`、`ToolMessage`          | -                               | `messages/tool.py`     |
| `ChatMessageChunk`     | `BaseMessageChunk`、`ChatMessage`          | -                               | `messages/chat.py`     |
| `FunctionMessageChunk` | `BaseMessageChunk`、`FunctionMessageChunk` | -                               | `messages/function.py` |
#### 2. `BaseMessage`类与`BaseMessageChunk`类
`BaseMessage`是**所有非流式输出消息的唯一基类**，它继承于Pydantic中`BaseModel`的自定义子类`Serializable`，源码中的定义为：
```python
class BaseMessage(Serializable):
	content: str | list[str | dict]
	additional_kwargs: dict = Field(default_factory=dict)
	response_metadata: dict = Field(default_factory=dict)
	type: str
	name: str | None = None
	id: str | None = Field(default=None, coerce_numbers_to_str=True)
```
另外`BaseMessageChunk`继承于`BaseMessage`，并且只添加了一个`__add__`方法用于拼接。

#### 3. `Message`类中的token统计
**实例1：** 使用deepseek模型创建智能体的返回结果：
```json
{
	'messages': [
		AIMessage(
			content='你好！我是你的智能助手，随时准备为你提供帮助。无论是回答问题、解决问题、提供建议，还是陪你聊天，我都在这里！请告诉我你需要什么帮助吧！ 😊', 
			additional_kwargs={'refusal': None}, 
			response_metadata={
				'token_usage': {
					'completion_tokens': 38, 
					'prompt_tokens': 7, 
					'total_tokens': 45, 
					'completion_tokens_details': None, 
					'prompt_tokens_details': {
						'audio_tokens': None, 
						'cached_tokens': 0
					}, 
					'prompt_cache_hit_tokens': 0, 
					'prompt_cache_miss_tokens': 7
				}, 
				'model_provider': 'deepseek', 
				'model_name': 'deepseek-chat', 
				'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 
				'id': '3f0bdd13-64fe-4b34-8a1f-d5ff1885e60b', 
				'finish_reason': 'stop', 
				'logprobs': None
			}, 
			id='lc_run--019bb097-9a26-78f2-8d4e-21e705f51bdc-0', 
			tool_calls=[], 
			invalid_tool_calls=[], 
			usage_metadata={
				'input_tokens': 7, 
				'output_tokens': 38, 
				'total_tokens': 45, 
				'input_token_details': {
					'cache_read': 0
				}, 
				'output_token_details': {}
			}
		)
	]
}
```
分解一下这个`AIMessage`的结构：在源码定义中它新增了`tool_calls`、`invalid_tool_calls`、`usage_metadata`字段，并将基类的`type`字段赋值为`"ai"`（不显示）。这就是上面返回结果的最后三个字段。并且能够注意到，和消息耗费token相关的字段有`response_metadata`和`usage_metadata`。且两个字段中token计数重合，
**实例2：** 结合多轮工具调用返回的结果，它包含了最常用的消息类型（Human -> AI -> Tool -> AI）：
```json
{
'messages': [
	HumanMessage(
		content='请帮我查询2024年诺贝尔物理学奖得主是谁？', 
		additional_kwargs={}, 
		response_metadata={}, 
		id='9e05fe69-413e-430a-8c09-cfc91416a589'
	), 
	
	%% --- 第一次调用工具解决问题 --- %%
	AIMessage(
		content='我来帮您查询2024年诺贝尔物理学奖得主的信息。', 
		additional_kwargs={'refusal': None}, 
		response_metadata={
			'token_usage': {
				'completion_tokens': 99, 
				'prompt_tokens': 1864, 
				'total_tokens': 1963, 
				'completion_tokens_details': None, 
				'prompt_tokens_details': {
					'audio_tokens': None, 
					'cached_tokens': 0
				}, 
				'prompt_cache_hit_tokens': 0, 
				'prompt_cache_miss_tokens': 1864
			}, 
			'model_provider': 'deepseek', 
			'model_name': 'deepseek-chat', 
			'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 
			'id': 'e743c6a3-5c2e-425b-b152-466bf44e7a3c', 
			'finish_reason': 'tool_calls', 
			'logprobs': None
		}, 
		id='lc_run--019bb0a3-0a51-7092-9b52-e8f4aa9566ee-0', 
		tool_calls=[{
			'name': 'tavily_search', 
			'args': {
				'query': '2024年诺贝尔物理学奖得主', 
				'search_depth': 'advanced', 
				'time_range': 'year'
			}, 
			'id': 'call_00_6RJOId6kxsYmMbiUwvoSkv4m', 
			'type': 'tool_call'
		}], 
		invalid_tool_calls=[], 
		usage_metadata={
			'input_tokens': 1864, 
			'output_tokens': 99, 
			'total_tokens': 1963, 
			'input_token_details': {
				'cache_read': 0
			}, 
			'output_token_details': {}
		}
	), 
	
	ToolMessage(
		content='{
			"query": "2024年诺贝尔物理学奖得主", 
			"follow_up_questions": null, 
			"answer": null, 
			"images": [], 
			"results": [
				{
					"url": "https://bimonthly.ps-taiwan.org/articles/67bd3bf214c2b3f65f312b8f", 
					"title": "2024年諾貝爾物理獎--- 人工智慧上的奠基與發明", 
					"content": "國立中央大學物理系 黎璧賢教授2025年2月25日2857\\n\\n圖片來源：www.pexels.com\\n\\n分享：\\n\\n2024年諾貝爾物理獎頒給美國普林斯頓大學的約翰‧霍普菲爾德(John J. Hopfield，91歲)，以及加拿大多倫多大學的傑佛瑞‧辛頓(Geoffrey E. Hinton，76歲)，「表揚他們在人工智慧與機器學習上的奠基與發明」。最近，生成式人工智慧變得越來越流行和易於使用，對科學和日常應用產生了巨大並深遠的影響。儘管電腦無法思考，但人工智慧(Artificial Intelligence，AI)機器可以模仿記憶和學習等功能，而機器學習則一直主導著人工智慧的成長與發展。過去十五到二十年，機器學習的發展呈現爆炸性的成長主要是利用一種稱為人工神經網絡(Artificial Neural Network，ANN)的工具。巧合的是今年是易辛模型 【截断】", 
					"score": 0.90849644, 
					"raw_content": null
				}, 
				{
					"url": "https://www.jfdaily.com/news/detail?id=995353", 
					"title": "刚刚揭晓！三名科学家获诺贝尔物理学奖", 
					"content": "### 关注我们\\n\\n官方微博\\n\\n分享到微信朋友圈\\n\\n打开微信，点击底部的“发现”，使用 “扫一扫” 即可将网页分享到我的朋友圈。\\n\\n# 刚刚揭晓！三名科学家获诺贝尔物理学奖\\n\\n科科哒\\n2025-10-07 05:53\\n\\n来源：上观新闻\\n作者：新民晚报 郜阳\\n\\n稍后带来更多解读\\n\\n {{brTitle}}\\n\\n01北京时间6日17时45分，2025年度诺贝尔物理学奖揭晓。\\n\\n02今年物理学奖授予约翰·克拉克、米歇尔·H·德沃雷、约翰·M·马丁尼斯，以表彰他们发现电路中的宏观量子力学隧穿效应和能量量子化。【截断】", 
					"score": 0.8812988, 
					"raw_content": null
				}
			], 
			"response_time": 3.45, 
			"request_id": "b2caa602-0ee1-47a7-ba83-2d3cb5841dde"
		}', 
		name='tavily_search', 
		id='3925e83a-33d2-49fc-a33f-c215c70d2dc1',
		tool_call_id='call_00_6RJOId6kxsYmMbiUwvoSkv4m'),
	
	%% --- 第二次调用工具解决问题 --- %% 
	AIMessage(
		content='让我再搜索一些更权威的官方信息来确认。', 
		additional_kwargs={'refusal': None}, 
		response_metadata={
			'token_usage': {
				'completion_tokens': 98, 
				'prompt_tokens': 4362, 
				'total_tokens': 4460, 
				'completion_tokens_details': None, 
				'prompt_tokens_details': {
					'audio_tokens': None, 
					'cached_tokens': 1920
				}, 
				'prompt_cache_hit_tokens': 1920, 
				'prompt_cache_miss_tokens': 2442
			}, 
			'model_provider': 'deepseek', 
			'model_name': 'deepseek-chat', 
			'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 
			'id': '159649fe-18e3-46a2-adf7-fe9be742ee1d', 
			'finish_reason': 'tool_calls', 
			'logprobs': None
		}, 
		id='lc_run--019bb0a3-2ba7-7173-87b5-d680f66574d5-0', 
		tool_calls=[{
			'name': 'tavily_search', 
			'args': {
				'query': '2024 Nobel Prize in Physics winners official announcement', 
				'search_depth': 'advanced', 
				'time_range': 'year'
			}, 
			'id': 'call_00_ste2P3uecFQELA77gGmjXFtZ', 
			'type': 'tool_call'
		}], 
		invalid_tool_calls=[], 
		usage_metadata={
			'input_tokens': 4362, 
			'output_tokens': 98, 
			'total_tokens': 4460, 
			'input_token_details': {'cache_read': 1920}, 
			'output_token_details': {}
		}
	),
	 
	ToolMessage(
		content='{
			"query": "2024 Nobel Prize in Physics winners official announcement",
			"follow_up_questions": null, 
			"answer": null, 
			"images": [], 
			"results": [
				{
					"url": "https://blog.google/【截断】", 
					"title": "Googler Michel 【截断】", 
					"content": "x.com Facebook 【截断】", 
					"score": 0.8820324, 
					"raw_content": null
				}, {
					"url": "https://www.nobelprize.org/【截断】", 
					"title": "Nobel Prize Dialogue", 
					"content": "【截断】", 
					"score": 0.84918517, 
					"raw_content": null
				}
			], 
			"response_time": 2.07, 
			"request_id": "07b2c776-dec7-4ad8-95de-8b5c68b5eefe"
		}', 
		name='tavily_search', 
		id='09004523-6baf-47f9-90af-c0d2c407b188', 
		tool_call_id='call_00_ste2P3uecFQELA77gGmjXFtZ'), 
	
	%% --- 第三次调用工具解决问题 --- %%
	AIMessage(
		content='让我再搜索一下更具体的官方信息。', 
		additional_kwargs={'refusal': None}, 
		response_metadata={
			'token_usage': {
				'completion_tokens': 101, 
				'prompt_tokens': 5078, 
				'total_tokens': 5179, 
				'completion_tokens_details': None, 
				'prompt_tokens_details': {
					'audio_tokens': None, 
					'cached_tokens': 4416
				}, 
				'prompt_cache_hit_tokens': 4416, 
				'prompt_cache_miss_tokens': 662
			}, 
			'model_provider': 'deepseek', 
			'model_name': 'deepseek-chat', 
			'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 
			'id': '6c7411ea-d0db-47b7-bfd1-b1b5ce6afeab', 
			'finish_reason': 'tool_calls', 
			'logprobs': None
		}, 
		id='lc_run--019bb0a3-47e1-79b1-919a-3d2933014513-0', 
		tool_calls=[{
			'name': 'tavily_search', 
			'args': {
				'query': '"2024 Nobel 【截断】', 
				'search_depth': 'advanced', 
				'time_range': 'year'
			}, 
			'id': 'call_00_7ywqfC5G9migCbsxl2b68L14', 
			'type': 'tool_call'
		}], 
		invalid_tool_calls=[], 
		usage_metadata={
			'input_tokens': 5078, 
			'output_tokens': 101, 
			'total_tokens': 5179, 
			'input_token_details': {'cache_read': 4416}, 
			'output_token_details': {}
		}
	),
			 
	ToolMessage(
		content='{
			"query": "2024 Nobel Prize 【截断】", 
			"follow_up_questions": null, 
			"answer": null, 
			"images": [], 
			"results": [
				{
					"url": "https://www.nobelprize.org/【截断】", 
					"title": "The Nobel Foundation annual review 2024", 
					"content": "Foundation 【截断】", 
					"score": 0.65861565, 
					"raw_content": null
				}, {
					"url": "https://www.youtube.com/watch?v=XDE9DjpcSdI", 
					"title": "Nobel Prize lecture: 【截断】", 
					"content": "# Nobel Prize lecture: 【截断】", 
					"score": 0.63750535, 
					"raw_content": null
				}
			], 
			"response_time": 2.96, 
			"request_id": "a6c7c5b5-9253-4a41-a107-8679b35f2def"
		}', 
		name='tavily_search', 
		id='60d7f77e-0ab3-4a5f-af07-8a3c0d26a0c5',
		tool_call_id='call_00_7ywqfC5G9migCbsxl2b68L14'
	), 
	
	%% --- 最终回答 --- %%
	AIMessage(
		content='根据搜索结果，我可以为您提供2024年诺贝尔物理学奖得主的准确信息：\n\n## 2024年诺贝尔物理学奖得主\n\n**获奖者：**\n1. **约翰·J·霍普菲尔德（John J. Hopfield）** - 美国普林斯顿大学教授，91岁\n2. **杰弗里·E·辛顿（Geoffrey E. Hinton）** - 加拿大多伦多大学教授，76岁\n\n**获奖理由：**\n"表彰他们在人工神经网络机器学习方面的奠基性发现和发明"（"for foundational discoveries and inventions that enable machine learning with artificial neural networks"）\n\n**主要贡献：**\n1. **约翰·霍普菲尔德**：发明了霍普菲尔德网络（Hopfield network），这是一种基于物理学中伊辛模型（Ising model）的人工神经网络，能够存储和重建模式，在图像分析等领域有重要应用。\n\n2. **杰弗里·辛顿**：以霍普菲尔德网络为基础，开发了玻尔兹曼机（Boltzmann machine），这是一种能够学习识别特定数据类型特征元素的新型网络。他的工作推动了当前机器学习的爆炸性发展。\n\n**重要意义：**\n- 这两位科学家的研究将物理学概念（特别是统计力学）应用于人工智能领域\n- 他们的工作为现代机器学习奠定了基础\n- 人工神经网络现在已成为物理学、科学和技术各个领域建模和分析的强大工具\n\n值得注意的是，这是诺贝尔物理学奖首次颁发给人工智能/机器学习领域的研究者，反映了人工智能技术在当代科学中的重要性日益增长。', 
		additional_kwargs={'refusal': None}, 
		response_metadata={
			'token_usage': {
				'completion_tokens': 311, 
				'prompt_tokens': 5908, 
				'total_tokens': 6219, 
				'completion_tokens_details': None, 
				'prompt_tokens_details': {
					'audio_tokens': None, 
					'cached_tokens': 5120
				}, 
				'prompt_cache_hit_tokens': 5120, 
				'prompt_cache_miss_tokens': 788
			}, 
			'model_provider': 'deepseek', 
			'model_name': 'deepseek-chat', 
			'system_fingerprint': 'fp_eaab8d114b_prod0820_fp8_kvcache', 
			'id': '8a76cc25-bceb-4975-b76d-85292e52c41d', 
			'finish_reason': 'stop', 
			'logprobs': None
		}, 
		id='lc_run--019bb0a3-66d5-78d0-b319-bee820c5197e-0', 
		tool_calls=[], 
		invalid_tool_calls=[], 
		usage_metadata={
			'input_tokens': 5908, 
			'output_tokens': 311, 
			'total_tokens': 6219, 
			'input_token_details': {'cache_read': 5120}, 
			'output_token_details': {}
			}
		)
	]
}
```
以上结果进行了三次工具调用，可以发现：
* `ToolMessage`虽然将结果加入下一轮调用的上下文中，**但并不提供token信息**，因为最终`AIMessage`会统计包含了工具返回结果的总token消耗。**所以成本追踪只需参考`AIMessage`中的信息**。另外源码中对`response_metadata`字段的解释为（**仅继承，但不用**）：
```python
class ToolMessage(BaseMessage, ToolOutputMixin):
	additional_kwargs: dict = Field(default_factory=dict, repr=False)  
	"""Currently inherited from `BaseMessage`, but not used."""  
	response_metadata: dict = Field(default_factory=dict, repr=False)  
	"""Currently inherited from `BaseMessage`, but not used."""
```
* `AIMessage`新加入的`usage_metadata`字段与`BaseMessage`定义的`response_metadata`中的`token_usage`高度重合，并且有以下对应：

| `BaseMessage`的`response_metadata["token_usage"]` | `AIMessage`新增属性`usage_metadata`     |
| ------------------------------------------------ | ----------------------------------- |
| `prompt_tokens`                                  | `input_tokens`                      |
| `completion_tokens`                              | `output_tokens`                     |
| `total_tokens`                                   | `total_tokens`                      |
| `prompt_tokens_details["cached_tokens"]`         | `input_token_details["cache_read"]` |
| `prompt_cache_hit_tokens`                        | `input_token_details["cache_read"]` |
目前只分析`input_tokens`、`output_tokens`、`total_tokens`，后续分别称为输入token、输出token、总token。然后对四个AI消息对进行token数据对比：

|                  | 输入token | 输出token | 总token |
| ---------------- | ------- | ------- | ------ |
| 第一次AI消息（第一次工具调用） | 1864    | 99      | 1963   |
| 第二次AI消息（第二次工具调用） | 4362    | 98      | 4460   |
| 第三次AI消息（第三次工具调用） | 5078    | 101     | 5179   |
| 第四次AI消息（输出结果）    | 5908    | 311     | 6219   |
从这三次调用可以看出LangChain框架进行的token计算的机制：
1. 首次调用虽然问题简单：“你好，你叫什么？”，但是仍有1864的token输入，可能来源于提示词、工具列表（包含工具解释和它的参数文档）等等。
2. 第二次的token数飙升，达到4362。这来源于第一次工具调用的返回结果+上下文
3. 第三次的token数只增不减，可能因为没有配备上下文管理器，导致只会不断地加入新信息
4. 第四次的输出token与前三次不同，因为前三次的输出是用作Function Call，有固定的格式，且调用的工具相同，第四次输出是最终结果。另外，输入token数仍在在增加。

从这个输出结果的分析中可以发现以下问题：
**Q：** 多轮的调用大模型必定会造成成本上涨，目前只有三轮的工具调用，其总token消耗量就达到了接近两万，甚至工具调用（网页搜索）只要求输出了两个页面。如果在生产环境下，开销一定会更大，所以缩减token消耗是非常重要的。目前已知**上下文管理器可以缓解这个问题**，但是如果**引入多智能体分工协作，是否一定程度上可以解决这个问题**？
**A：** 传统多轮调用Agent的token成本是线性，甚至指数级增长的。多智能体工作流是一种前沿的解决该问题的方法，它的核心思路是**分工与记忆隔离**：由”调度员“Agent分析任务，将子任务分发给专门的”执行员“Agent。**每个执行员只需关注自己的上下文，避免了全局历史堆积**；另外每个Agent可以拥有独立的短期记忆，可以**通过各种方法压缩上下文，而非传递原始长度的文本**。

**Q：** 实现回调或中间件监控程序执行流程的方法很多，FastAPI有处理HTTP请求和发送HTTP响应之间作用的中间件，LangChain也有能够在智能体工作流之间作用的中间件，回调函数、日志提供的装饰器都能够实现监控和追踪函数内部执行流程的功能。**成本追踪应该处于哪一层**？
**A：** 由以上消息列表中的AI消息可以看出，大模型的每次调用所产生的`AIMessage`中的**token计数并非累加的，而是仅限于单次调用的**。所以如果只是在`invoke()`层面调用是粗粒度的，它只会得到最后一次调用的token消耗，造成成本误算。**所以需要在模型每次调用的层面进行成本追踪，可以使用LangChain提供的回调机制（`Callback`）成本追踪嵌入LLM的调用环节**。

### 二、LangChain的`Callback`机制
LangChain的中间件由`Callback`机制支持，它允许开发人员在应用执行的关键节点（如LLM调用前、工具执行后）插入自定义逻辑，用于监控、记录、调试，而无需改动核心业务代码。它的工作流程为：Agent执行`invoke()`方法 -> 生成LangChain可运行对象（`Runnable`） -> 由**回调管理器`CallbackManager`将事件分发给其他处理器**（如回调处理器`BaseCallbackHandler`、日志处理器），再由这些处理器内部的自定义逻辑完成监控、记录、调试等功能。
**Q：** 为什么感觉`Callback`机制和日志收集器`Logger`收集日志后分发给各种执行器`Handler`的机制很像？
**A：** 虽然存在侧重点的差异（**日志系统侧重异步记录，不改变代码执行；而同步的`Callback`则可能影响执行**），这两者是**同一种设计模式（观察者模式）在不同场景下的应用**：
* 同样有**发布事件的管理组件**（`Logger`、`CallbackManager`）；
* 同样有**被分发的消息**（`logging.info("msg")`这种日志调用、`on_llm_start`这些框架触发的事件）；
* 同样有**能够订阅并处理事件的执行器**（`Handler`、`BaseCallbackHandler`）；
* 同样有**将执行器注册到管理器的方法**（`logger.addHandler()`、`callbacks=[]`）；
* 同样有**一对多的数据流**（`Logger`->多个`Handler`、`CallbackManager`->多个`Handler`）。
所以可以把`CallbackManager`理解为LangChain内部一个**专有的”事件日志记录器“**，它不断地发布”LLM调用开始“、”工具调用结束“等事件，而`BaseCallbackHandler`就能够订阅这些事件、并执行特定动作。
#### 相关类的源码分析
##### 1. `langchain_core/callbacks/base.py`文件结构
该文件为管理器抽象基类`BaseCallbackManager`和执行器抽象基类（同步`BaseCallbackHandler`；异步`AsyncCallbackHandler`）的定义。虽然该文件中存在9个类，但它们的关系很简单：
* **6个`Mixin`类**：只提供特定功能，但不是为了独立使用，而是为了让其他类继承这些功能，包括6个：`RetrieverManagerMixin`、`LLMManagerMixin`、`ChainManagerMixin`、`ToolManagerMixin`、`CallbackManagerMixin`、`RunManagerMixin`，它们分别提供了六个方面的功能；
* **2个执行器基类**：继承了这六个`Mixin`类的`BaseCallbackHandler`（回调执行器基类）、继承了`BaseCallbackHandler`的`AsyncCallbackHandler`（异步回调执行器）
* **1个管理器基类**：继承了`CallbackManagerMixin`的`BaseCallbackManager`
###### 1) `RetrieverManagerMixin`
负责为检索器提供回调管理，包含2个方法：
* `on_retriever_error`：检索知识库出错事件；
* `on_retriever_end`：检索知识库完成时事件。
###### 2) `LLMManagerMixin`
负责为LLM提供回调管理，包含3个方法：
* `on_llm_new_token`：仅限流式输出时起作用，生成新token的事件。
```python
"""Run on new output token. Only available when streaming is enabled.  
For both chat models and non-chat models (legacy LLMs)"
```
* `on_llm_end`：调用LLM完成事件；
* `on_llm_error`：调用LLM出错事件。
###### 3) `ChainManagerMixin`
负责为链提供回调管理，包含4个方法：
* `on_chain_end`：调用链结束事件；
* `on_chain_error`：调用链出错事件；
* `on_agent_action`：Agent执行动作事件；
* `on_agent_finish`：Agent执行动作完成事件。
###### 4) `ToolManagerMixin`
负责为工具提供回调管理，包含2个方法：
* `on_tool_end`：调用工具结束；
* `on_tool_error`：调用工具出错。
###### 5) `CallbackManagerMixin`
回调管理器的功能，包含5个方法：
* `on_llm_start`：开始调用大模型事件（单次调用）；
* `on_chat_model_start`：开始调用聊天模型事件；
* `on_retriever_start`：开始调用检索器事件；
* `on_chain_start`：开始调用链事件；
* `on_tool_start`：开始调用工具事件；
###### 6) `RunManagerMixin`
运行管理器的功能，包含3个方法：
* `on_text`：运行任意文本；
* `on_retry`：重试事件；
* `on_custom_event`：重写它以自定义一个事件。
###### 7) `BaseCallbackHandler`
LangChain的回调执行器基类，它包含了两个字段`raise_error`（是否发起异常）和`run_inline`（是否启用同步，默认为`False`异步执行），以及7个`ignore_xxx() -> bool`方法用于选择性忽略一些回调：`llm`、`retry`、`chain`、`agent`、`retriever`、`chat_model`、`custom_event`
###### 8) `AsyncCallbackHandler`
LangChain的异步回调执行器基类，虽然占据了将近一半的文件内容，但是它内部只是使用`async def`把 1) ~ 6) 的所有事件重写了一遍。
###### 9) `BaseCallbackManager`
LangChain的基类回调管理器，包含了各种对`metadata`、`tags`、`handler`的添加、移除、设置操作方法，和如`merge`、`copy`这种对管理器的合并、复制方法。

其中，在源码中可以发现每个事件存在两个`UUID`类型的参数：`run_id`和`parent_run_id`。因为在Agent调用中，比如Agentic RAG，它在进行检索时需要调用工具，那么就有这样的事件流程： `on_agent_action` -> `on_tool_start` -> `on_retriever_start` -> `on_retriever_end` -> `on_tool_end` -> `on_agent_finish`，可见事件的执行是有层级的，这个过程会像数一样展开，使**以`run_id`和`parent_run_id`构成了一个”父节点表示法“的事件树**。
在”观察者模式“的视角下：运行中的`LLM`、`Chain`、`Retriever`、`Agent`、`Tool`等组件就是这个系统所**观察的对象**；子类`CallbackHanler`就是观察某几个组件的事件的**观察者**；子类`CallbackManager`负责集成所有执行器，并在事件发生时，及时地发送给它们，它是**作为调度中心**存在的。
**总之这个基类文件做的事情就只有两个：一个是按功能使用6个`Mixin`类定义了一堆事件（LangSmith可视化测试中模型图的边），涵盖了agent调用这套完整过程中各环节；二是使用管理器和执行器将这些事件按需求集成。**

